{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self,token_to_idx=None,add_unk=True,unk_token=\"<UNK>\"):\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "\n",
    "        self.token_to_idx= token_to_idx\n",
    "        self.idx_to_token={idx:token for token,idx in token_to_idx.items()}\n",
    "        self.add_unk=add_unk\n",
    "        self.unk_token=unk_token\n",
    "        self.unk_index=-1\n",
    "        if add_unk:\n",
    "            self.unk_index= self.add_token(unk_token) \n",
    "\n",
    "    def to_serailizable(self):\n",
    "        return {\n",
    "            \"token_to_idx\": self.token_to_idx,\n",
    "            \"add_unk\":self.add_unk,\n",
    "            \"unk_token\":self.unk_token\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serailizable(cls,contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self,word):\n",
    "\n",
    "        if word not in self.token_to_idx:\n",
    "            self.token_to_idx[word]=len(self.token_to_idx)\n",
    "            self.idx_to_token[self.token_to_idx[word]]=word\n",
    "        return self.token_to_idx[word]\n",
    "\n",
    "    def lookup_token(self,token):\n",
    "        \n",
    "        if token not in self.token_to_idx:\n",
    "            return self.token_to_idx.get(token,self.unk_token)\n",
    "        else:\n",
    "            return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_idx(self,idx):\n",
    "\n",
    "        if idx not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % idx)\n",
    "        else:\n",
    "            return self.idx_to_token[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "\n",
    "    def __init__(self,text_vocab,rating_vocab):\n",
    "        self.text_vocab=text_vocab\n",
    "        self.rating_vocab=rating_vocab\n",
    "\n",
    "    def vectorize(self,text):\n",
    "        \n",
    "        one_hot=np.zeros(len(self.text_vocab),dtype=np.float32)\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word not in string.punctuation:\n",
    "                if word in self.text_vocab.token_to_idx:\n",
    "                    one_hot[self.text_vocab.lookup_token(word)]=1\n",
    "\n",
    "        return one_hot\n",
    "    \n",
    "    def vectorize_y(self,rating):   \n",
    "        one_hot=np.zeros(len(self.rating_vocab),dtype=np.float32)\n",
    "        one_hot[self.rating_vocab.lookup_token(rating)]=1\n",
    "        return one_hot\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls,review_df,cutoff=25):\n",
    "\n",
    "        text_vocab=Vocabulary()\n",
    "        rating_vocab=Vocabulary(add_unk=False)\n",
    "\n",
    "        for rating in review_df.stars:\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        words=Counter()\n",
    "\n",
    "        for texts in review_df.text:\n",
    "            for word in texts.split():\n",
    "                if word not in string.punctuation:\n",
    "                    words[word] +=1\n",
    "\n",
    "        for word,counts in words.items():\n",
    "            if counts > cutoff:\n",
    "                text_vocab.add_token(word)\n",
    "\n",
    "        return cls(text_vocab,rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serailizable(cls,contents):\n",
    "\n",
    "        text_vocab=Vocabulary.from_serailizable(contents[\"review_vocab\"])\n",
    "        rating_vocab=Vocabulary.from_serailizable(contents[\"rating_vocab\"])\n",
    "\n",
    "        return cls(text_vocab=text_vocab,rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serailizable(self):\n",
    "\n",
    "        return {\n",
    "                \"text_vocab\": self.text_vocab.to_serailizable(),\n",
    "                \"rating_vocab\":self.rating_voca.to_serailizable()\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yelp(Dataset):\n",
    "\n",
    "    def __init__(self,target_df,vectorizer,trainable):\n",
    "        self.target_df=target_df\n",
    "        self.vectorizer=vectorizer\n",
    "        self.trainable=trainable\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "\n",
    "    @classmethod\n",
    "    def creat_new(cls,path,trainable):\n",
    "        target=pd.read_csv(path)\n",
    "        if trainable:\n",
    "            vectorizer=Vectorizer.from_dataframe(target)\n",
    "        else:\n",
    "            vectorizer=None\n",
    "        return cls(target,vectorizer,trainable)\n",
    "\n",
    "    def set_vectorizer(self,vectorizer):\n",
    "        self.vectorizer=vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row=self.target_df.iloc[index]\n",
    "        text_vector= \\\n",
    "            self.vectorizer.vectorize(row.text)\n",
    "        rating_vector=\\\n",
    "            self.vectorizer.vectorize_y(row.stars)\n",
    "        \n",
    "        return {\n",
    "            \"x_data\" : text_vector,\n",
    "            \"y_target\": rating_vector        \n",
    "            }\n",
    "    \n",
    "    def get_num_batches(self,batch_size):\n",
    "\n",
    "        return len(self)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset,batch_size,shuffle=True,drop_last=True,device=\"cpu\"):\n",
    "\n",
    "    dataloader= DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict={}\n",
    "        for name,tensor in data_dict.items():\n",
    "            out_data_dict[name]=data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(in_features=num_features,out_features=5)\n",
    "\n",
    "    def forward(self,x_in,apply_sigmoid=False):\n",
    "\n",
    "        y_out=self.fc1(x_in)#.squezze\n",
    "        if apply_sigmoid:\n",
    "            y_out=F.sigmoid(y_out)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "frequency_cutoff=25,\n",
    "model_state_file='model.pth',\n",
    "review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "save_dir='model_storage/ch3/yelp/',\n",
    "vectorizer_file='vectorizer.json',\n",
    "batch_size=128,\n",
    "early_stopping_criteria=5,\n",
    "learning_rate=0.001,\n",
    "num_epochs=100,\n",
    "seed=1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(arge):\n",
    "    return {\n",
    "        \"epoch_index\":0,\n",
    "        \"train_loss\":[],\n",
    "        \"train_acc\":[],\n",
    "        \"val_loss\":[],\n",
    "        \"val_acc\":[],\n",
    "        \"test_loss\":-1,\n",
    "        \"test_acc\":-1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state=make_train_state(args)\n",
    "\n",
    "args.device=device\n",
    "train_dataset=Yelp.creat_new(\"train_dataset.csv\",trainable=True)\n",
    "test_dataset=Yelp.creat_new(\"test_dataset.csv\",trainable=False)\n",
    "valid_dataset=Yelp.creat_new(\"valid_dataset.csv\",trainable=False)\n",
    "\n",
    "vectorizer=train_dataset.get_vectorizer()\n",
    "test_dataset.set_vectorizer(vectorizer)\n",
    "valid_dataset.set_vectorizer(vectorizer)\n",
    "\n",
    "classifier= ReviewClassifier(num_features=len(vectorizer.text_vocab))\n",
    "classifier.to(device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    n_correct = sum(sum(sum([y_pred.detach()-y_target])))\n",
    "    return n_correct / (len(y_pred)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state[\"epoch_index\"]=epoch_index\n",
    "    batch_generator=generate_batches(train_dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    classifier.train()\n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=classifier(x_in=batch_dict[\"x_data\"].float())\n",
    "        \n",
    "        loss=loss_func(y_pred,batch_dict[\"y_target\"].float())\n",
    "        loss_batch=loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_batch=compute_accuracy(y_pred,batch_dict[\"y_target\"])\n",
    "        running_acc += (acc_batch-running_acc)/(batch_index+1)\n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "    \n",
    "    batch_generator=generate_batches(valid_dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    classifier.eval()\n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        y_pred=classifier(xin=batch_dict[\"x_data\"].float())\n",
    "        loss=loss_func(y_pred,batch_dict[\"y_target\"].float())\n",
    "        loss_batch=loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        acc_batch=compute_accuracy(y_pred,batch_dict[\"y_target\"])\n",
    "        running_acc += (acc_batch-running_acc)/(batch_index+1)\n",
    "        \n",
    "    train_state[\"valid_loss\"].append(running_loss)\n",
    "    train_state[\"valid_acc\"].append(running_acc)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
